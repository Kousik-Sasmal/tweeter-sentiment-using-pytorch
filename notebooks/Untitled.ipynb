{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340bd39e-b706-4c48-86ee-27489d8a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Due to warning when initializing the \"spacy\" tokenizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow logging\n",
    "logging.getLogger('tensorflow').disabled = True  # disable tensorflow warning messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5e459a-e2cf-4ad8-aafe-a1f076d41349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c4cd56-f5da-4ce2-807d-b89a869abf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "df_valid = pd.read_csv('artifacts/valid_cleaned.csv')\n",
    "\n",
    "\n",
    "# limit df\n",
    "df = df[:10000]\n",
    "\n",
    "df_valid = df_valid[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989b6a6d-6a7b-4728-926f-d50ebb2156e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb09bcbb-be27-4fe6-9340-638052b17247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset class for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self,text,target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, int]:\n",
    "        \"\"\"Return item at given index.\"\"\"\n",
    "       \n",
    "        text = self.text[index]\n",
    "        target = self.target[index]\n",
    "        \n",
    "        return text, target\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955910c4-be17-4549-af8d-10447a2d0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "test_data = CustomDataset(df_valid['tweet'],df_valid['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c716d85-9fcc-4686-9da2-4c4b30b7fc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('im getting on borderlands and i will murder you all ,', 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a6c890-9420-44e4-bf4f-41bfd3b283ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary  \n",
    "    \n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        text = text.lower()\n",
    "        \n",
    "        yield tokenizer(text)\n",
    "        \n",
    "        \n",
    "token_generator = yield_tokens(df['tweet'])\n",
    "        \n",
    "vocab = build_vocab_from_iterator(token_generator, specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4e5df-7a1f-4d4c-836c-765bb2c23a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b279ce00-7d94-426d-842f-6a2a662d4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    # Separate the texts and targets from the samples in a batch\n",
    "    texts, targets = zip(*samples)\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized_texts = [tokenizer(text.lower()) for text in texts]\n",
    "    \n",
    "    # Convert the tokenized texts to numerical values\n",
    "    text_indices = [torch.tensor(vocab(token)) for token in tokenized_texts]\n",
    "    \n",
    "    # Pad the text sequences to have the same length\n",
    "    padded_texts = torch.nn.utils.rnn.pad_sequence(text_indices, batch_first=True)\n",
    "    \n",
    "    # Convert the targets to tensors\n",
    "    target_tensor = torch.tensor(targets)\n",
    "    \n",
    "    return padded_texts, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc08bc9a-900d-4888-9c50-92d3d2ef3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_data,batch_size=3,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2beb45ad-498d-48eb-a8f4-56af6d085868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13])\n",
      "tensor([[   3,  140,  217,   15,   26,    7,    3,   88, 1141,   17,   37,    4,\n",
      "            0],\n",
      "        [   3,  128,  374,    6,    2, 2867,    7,    3,   88,  731,   17,   37,\n",
      "            4],\n",
      "        [   3,  140,  217,   15,   26,    7,    3,   88,  731,   17,   37,    4,\n",
      "            0]])\n",
      "torch.Size([3, 14])\n",
      "tensor([[   3,  140,  374,   15,   26,    7,    3,   88, 1141,   17,   37,    4,\n",
      "            0,    0],\n",
      "        [   3,  140,  217,   15,   26,   63,    7,    3,   88, 1141,   17,   38,\n",
      "           37,    4],\n",
      "        [   3,  140,  217,  221,   26,    7,    3,   85, 1141,   17,   37,    4,\n",
      "            0,    0]])\n",
      "torch.Size([3, 62])\n",
      "tensor([[  25,    3,  800,    8,  388,  292,  432,  228,   10,  101,    1,    1,\n",
      "            1,   50,   17,   39,   44,  170,    3,  128,    8,  412,  206,  541,\n",
      "            7,  968,   12,   55,   11,   21,  229,  431,    1,   25,    3,  647,\n",
      "            6,  188,  417,    8, 5416,   10,   21,  237,    1,    1,  138,   12,\n",
      "            2,  821, 1710, 5401,    2, 4766,    3,  212,  805,  485,    5, 6096,\n",
      "            0,    0],\n",
      "        [  25,    3,  800,    8,  958,   11,  292,  376,  228,   10,  101,   18,\n",
      "           50,   17,   39,   44,  170,   23,    3,   61,    8,  412,   29,   26,\n",
      "          541,    7,  968,   12,   55,   11,   21,  229,  431,    4,    3,  647,\n",
      "            6,  188,    8, 5416,   10,   21,  237,   22,  138,   32,    2,  821,\n",
      "         2388,  892,    6,    2, 4766,    3,  212,  805,   31,  101,    5,  319,\n",
      "           13, 9412],\n",
      "        [  25,    3,  800,    8,  388,  292,  376,  228,   10,  101,   18,   50,\n",
      "           17,   39,   44,  170,    3,   61,    8,  412,   29,   26,  541,    7,\n",
      "          968,   12,   55,   11,   21,  229,  431,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in data_loader:\n",
    "    count+=1\n",
    "    print(i[0].shape)\n",
    "    print(i[0])\n",
    "    if count==3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c48b3-efed-463b-ae49-148398739b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73110753-28a6-4cd4-995b-31763c7b6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=20,collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data,batch_size=20,collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd7b826-3097-4d28-bdcc-ab43558bac11",
   "metadata": {},
   "source": [
    "### write a class for RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "988d1684-760a-4daa-807f-798049df939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "class RNNClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size,num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize the weights of the module\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #print('rnn output shape:',output.shape)\n",
    "        #print('rnn hidden shape:',hidden.shape)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking last output of RNN\n",
    "        #print('rnn last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7a570-d6d5-4fa4-98b3-26232b87450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1df50f3-f400-485d-8ebb-1d593deac289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10690"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "274de383-a06f-4881-a228-6e2baec87700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RNNClassify(vocab_size=VOCAB_SIZE,embed_dim=64,hidden_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b038de-fc69-430e-a869-b66938d36150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f5c6f0-4d77-4487-be39-2ad65be64d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "class LSTMClassify(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size,num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the LSTM layer\n",
    "        output, (hidden,cell) = self.lstm(embedded)\n",
    "        \n",
    "        \n",
    "        output = output[:, -1, :] \n",
    "        #print('LSTM last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab6836-a14c-4336-9264-ead3b10be239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcdcfdf5-806f-4d87-a8dc-2d3970e3caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes=4, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        lstm_output_size = hidden_size\n",
    "        if bidirectional:\n",
    "            lstm_output_size *= 2\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_output_size, num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        \n",
    "        # Pass the embedded input through the LSTM layer\n",
    "        output, (_, _) = self.lstm(embedded)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking the last output of the LSTM\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f51938-fc13-486a-b402-f96cd7e4454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMClassify(vocab_size=VOCAB_SIZE,embed_dim=64,hidden_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34253112-5030-416d-abfb-9df7da122945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75ab8f-3bd6-4ffb-82fc-8253b08bfc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69d432-cb4e-4263-b2f6-db58c2ff0d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a61cfb7-5e39-4602-b532-47a9ca20d620",
   "metadata": {},
   "source": [
    "### Write train-test loop for Mini-batch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11799ef8-9bae-4001-93f6-234a0d89c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740d7408-62e3-473e-813b-116f7ec02498",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_rnn.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "            \n",
    "        y_logits = model_rnn(X)\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (y_logits.argmax(1) == y).sum().item() / len(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)  # Calculate average loss per epoch\n",
    "    train_acc /= len(train_dataloader)  # Calculate average accuracy per epoch\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    \n",
    "    # evaluate model\n",
    "    with torch.inference_mode():\n",
    "        model_rnn.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "        for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
    "            \n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            \n",
    "            y_logits = model_rnn(X_test)\n",
    "\n",
    "            loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            test_preds = y_logits.argmax(dim=1)\n",
    "            test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "        \n",
    "        test_loss /= len(test_dataloader)  # Calculate average loss per epoch\n",
    "        test_acc /= len(test_dataloader)  # Calculate average accuracy per epoch\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "        \n",
    "        print('--'*25)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a45640-32ea-458b-9553-bac4bb9b85dc",
   "metadata": {},
   "source": [
    "### let's create a function for `train` and `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a79dfd7d-a90a-45ef-88ec-906277bf3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               optimizer:torch.optim,\n",
    "               device=device):\n",
    "    \n",
    "    train_loss,train_acc = 0,0\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        model.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "            \n",
    "        y_logits = model(X)\n",
    "\n",
    "        loss = loss_fn(y_logits, y)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_acc += (y_logits.argmax(1) == y).sum().item() / len(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "        \n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3ec695b6-7cc6-4e5a-813c-da0457bc0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               device=device):\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "        for batch, (X_test, y_test) in enumerate(dataloader):\n",
    "            \n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            \n",
    "            y_logits = model(X_test)\n",
    "\n",
    "            loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            test_preds = y_logits.argmax(dim=1)\n",
    "            test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "            \n",
    "        \n",
    "        test_loss /= len(dataloader)  \n",
    "        test_acc /= len(dataloader) \n",
    "\n",
    "\n",
    "    return test_loss,test_acc \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090bee0-128b-4468-9c78-8cfdd7ebb5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "59c71533-2aa1-4286-94b8-9b6a233435be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cd6afd954b46bba389837e82d43013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.3304, Train Accuracy: 0.3676\n",
      "Epoch 1/20, Test Loss: 1.4537, Test Accuracy: 0.2770\n",
      "--------------------------------------------------\n",
      "Epoch 2/20, Train Loss: 1.3429, Train Accuracy: 0.3691\n",
      "Epoch 2/20, Test Loss: 1.4543, Test Accuracy: 0.2770\n",
      "--------------------------------------------------\n",
      "Epoch 3/20, Train Loss: 1.3446, Train Accuracy: 0.3649\n",
      "Epoch 3/20, Test Loss: 1.4331, Test Accuracy: 0.2780\n",
      "--------------------------------------------------\n",
      "Epoch 4/20, Train Loss: 1.3269, Train Accuracy: 0.3793\n",
      "Epoch 4/20, Test Loss: 1.4564, Test Accuracy: 0.2790\n",
      "--------------------------------------------------\n",
      "Epoch 5/20, Train Loss: 1.3154, Train Accuracy: 0.3871\n",
      "Epoch 5/20, Test Loss: 1.4335, Test Accuracy: 0.2850\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test_step(model_lstm,test_dataloader,loss_fn)\n",
      "Cell \u001b[0;32mIn[199], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model,\n\u001b[1;32m      2\u001b[0m                dataloader,\n\u001b[1;32m      3\u001b[0m                loss_fn,\n\u001b[1;32m      4\u001b[0m                optimizer:torch\u001b[38;5;241m.\u001b[39moptim,\n\u001b[1;32m      5\u001b[0m                device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m      7\u001b[0m     train_loss,train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch,(X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      9\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:647\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m         warn_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor multiprocessing data-loading, this could be caused by not properly configuring the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterableDataset replica at each worker. Please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(warn_msg)\n\u001b[0;32m--> 647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/profiler.py:506\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# TODO: Too slow with __torch_function__ handling enabled\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/76410\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m    507\u001b[0m         torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit\u001b[38;5;241m.\u001b[39m_RecordFunction(record)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm.parameters(),lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_accuracy = train_step(model_lstm,train_dataloader,loss_fn,optimizer)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    test_loss, test_accuracy = test_step(model_lstm,test_dataloader,loss_fn)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "    print('--'*25)   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840fbb1-6abf-4a65-b39d-767b86cd79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b684d7-5cde-4f1b-b7c8-565951279cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f544c-43b4-46ff-9fc2-80cb89f5bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2dd2-aea6-46c1-a4ae-14e3d51a9e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1eb412-7251-4803-949b-f4f624e658eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9164c-78e8-4c62-a8d8-e770c6ceeca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb993220-98d8-49fb-8c97-95605eeb3b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ad575-9d80-4013-8809-3e463acb3c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f128d8c-98dd-4f2d-ae28-1bb87bc1b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in zip(train_data[0],train_data[1],train_data[2]):\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36e053-34b1-48d6-b0fd-ce2cbafaeab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5be67-88ec-4a4f-ab24-f1ad26453906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5cf99-2945-4542-9a5d-58e26736e86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094fbba-ba75-4280-9e46-e44407dbe04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f92343-4a5a-4db1-9027-6457d17a3101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a48d7-fc17-49b8-a6e6-b0da8a905595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8e62d-62ae-432a-be88-570067396cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92146b3e-fd61-4324-9269-0ef4705af7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba632852-c967-4223-b0d5-844fe1608ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72896b73-1559-4b72-8904-64e4b94df639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66efc5f-0fbc-450e-8a3f-6a0dafa82f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ad6e2-a7b4-4245-81cf-3d4e6c99db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
