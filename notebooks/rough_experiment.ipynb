{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89007f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Due to warning when initializing the \"spacy\" tokenizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow logging\n",
    "logging.getLogger('tensorflow').disabled = True  # disable tensorflow warning messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f59bdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d44f096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So I spent a couple of hours doing something f...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So I spent a few hours doing something for fun...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010 So I spent a few hours making something f...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>was</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Live Rock - Hard music La la Varlope, RARE &amp; t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-Hard like me, RARE LONDON DE, HANDSOME 2011,...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>that was the first borderlands session in a lo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this was the first Borderlands session in a lo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet sentiment  label\n",
       "0   im getting on borderlands and i will murder yo...  Positive      3\n",
       "1   I am coming to the borders and I will kill you...  Positive      3\n",
       "2   im getting on borderlands and i will kill you ...  Positive      3\n",
       "3   im coming on borderlands and i will murder you...  Positive      3\n",
       "4   im getting on borderlands 2 and i will murder ...  Positive      3\n",
       "5   im getting into borderlands and i can murder y...  Positive      3\n",
       "6   So I spent a few hours making something for fu...  Positive      3\n",
       "7   So I spent a couple of hours doing something f...  Positive      3\n",
       "8   So I spent a few hours doing something for fun...  Positive      3\n",
       "9   So I spent a few hours making something for fu...  Positive      3\n",
       "10  2010 So I spent a few hours making something f...  Positive      3\n",
       "11                                                was  Positive      3\n",
       "12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   Neutral      2\n",
       "13  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   Neutral      2\n",
       "14  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   Neutral      2\n",
       "15  Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...   Neutral      2\n",
       "16  Live Rock - Hard music La la Varlope, RARE & t...   Neutral      2\n",
       "17  I-Hard like me, RARE LONDON DE, HANDSOME 2011,...   Neutral      2\n",
       "18  that was the first borderlands session in a lo...  Positive      3\n",
       "19  this was the first Borderlands session in a lo...  Positive      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "\n",
    "# limit df to 5000\n",
    "df = df[:20]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7768ba-9895-4402-8eeb-2e95fffa9e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet        0\n",
       "sentiment    0\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380244aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accae582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cd46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>special shoutouts to microsoft excel 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Dang there goes my birthday present but maybe ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>It was ab fab seeing the 6 bungalows built in ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.7 million viewers? wtf? and cs:go has more t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet   sentiment  label\n",
       "0    I mentioned on Facebook that I was struggling ...  Irrelevant      0\n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...     Neutral      2\n",
       "2    @Microsoft Why do I pay for WORD when it funct...    Negative      1\n",
       "3    CSGO matchmaking is so full of closet hacking,...    Negative      1\n",
       "4    Now the President is slapping Americans in the...     Neutral      2\n",
       "..                                                 ...         ...    ...\n",
       "495          special shoutouts to microsoft excel 2013    Positive      3\n",
       "496  Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...  Irrelevant      0\n",
       "497  Dang there goes my birthday present but maybe ...    Positive      3\n",
       "498  It was ab fab seeing the 6 bungalows built in ...  Irrelevant      0\n",
       "499  1.7 million viewers? wtf? and cs:go has more t...     Neutral      2\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.read_csv('artifacts/valid_cleaned.csv')\n",
    "\n",
    "df_valid = df_valid[:500]\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150373c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33042f0c-ad3b-4170-9335-66a4879fe1f1",
   "metadata": {},
   "source": [
    "## Plan of action\n",
    "\n",
    "```\n",
    "1. Write a Custom Dataset\n",
    "\n",
    "2. collate function, that will make the data 'ready to fit' before feed the model\n",
    "\n",
    "3. model class\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cab86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('im getting on borderlands and i will murder you all ,', 3)\n",
      "('I am coming to the borders and I will kill you all,', 3)\n",
      "('im getting on borderlands and i will kill you all,', 3)\n",
      "('im coming on borderlands and i will murder you all,', 3)\n",
      "('im getting on borderlands 2 and i will murder you me all,', 3)\n",
      "('im getting into borderlands and i can murder you all,', 3)\n",
      "(\"So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg\", 3)\n",
      "(\"So I spent a couple of hours doing something for fun... If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters, I decided to make a wallpaper for my PC.. Here's the original picture compared to the creation I made:) Have fun! pic.twitter.com / mLsI5wf9Jg\", 3)\n",
      "(\"So I spent a few hours doing something for fun... If you don't know I'm a HUGE @ Borderlands fan and Maya is one of my favorite characters.\", 3)\n",
      "(\"So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg\", 3)\n",
      "(\"2010 So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg\", 3)\n",
      "('was', 3)\n",
      "('Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it/RMTrgF  ', 2)\n",
      "('Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it / RMTrgF', 2)\n",
      "('Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dfr.it / RMTrgF', 2)\n",
      "('Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME JACKPOT, Borderlands 1 (Xbox) dlvr.it/RMTrgF', 2)\n",
      "('Live Rock - Hard music La la Varlope, RARE & the POWERFUL, Live HANDSOME i JACKPOT, Borderlands 3 ( Sega Xbox ) dlvr. From it / e RMTrgF', 2)\n",
      "('I-Hard like me, RARE LONDON DE, HANDSOME 2011, Borderlands 3 (Xbox) dlvr.it/RMTrgF', 2)\n",
      "('that was the first borderlands session in a long time where i actually had a really satisfying combat experience. i got some really good kills', 3)\n",
      "('this was the first Borderlands session in a long time where i actually had a really satisfying fighting experience. i got some really good kills', 3)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 20 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m train_data \u001b[38;5;241m=\u001b[39m CustomDataset(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m],df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_data)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124;03m\"\"\"Return item at given index.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget[index]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, target\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 20"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset class for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self,text,target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, int]:\n",
    "        \"\"\"Return item at given index.\"\"\"\n",
    "       \n",
    "        text = self.text[index]\n",
    "        target = self.target[index]\n",
    "        \n",
    "        return text, target\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "\n",
    "# limit df to 20\n",
    "df = df[:20]\n",
    "\n",
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "train_iter = iter(train_data)\n",
    "\n",
    "for i in train_data:\n",
    "    print(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c5a67-72b1-4c70-8901-1ee26ce5c279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69b863-08b5-4bc6-9b2f-b88d6c36c749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3591f6-7bc6-4854-9f41-78398b1b50e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96520c0-4452-4518-8765-5a29c8d87b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625cc3-04ca-4fd4-8eab-f455d37233e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9b323-8dfc-40d6-837a-8236da1abaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(df['tweet'],df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89abdd-a7de-4700-b3c4-61d5d2cca281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data)\n",
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8ca47-56b2-4de4-ae36-603b7393a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a688161-23b5-4dfc-a5cc-4fb3ad05cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc91649-d560-4bf7-a4c9-f6ed7b9eee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7e0df-fd1b-421c-bad4-fc7dff75bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627d625-87d9-497a-a7ca-892b5bcb2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "train_iter = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d06266-110b-47c9-b4de-4f2fe564c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "train_iter = iter(train_data)\n",
    "\n",
    "for i in train_iter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9b396-04bf-4f6d-bafb-0d4096227386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc09ceb4-9afb-4cb4-8a7a-441765470b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 20 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer(text)\n\u001b[0;32m---> 46\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m     99\u001b[0m     counter\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m, in \u001b[0;36myield_tokens\u001b[0;34m(data_iter)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_tokens\u001b[39m(data_iter):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text,_ \u001b[38;5;129;01min\u001b[39;00m data_iter:\n\u001b[1;32m     42\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer(text)\n",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"Return item at given index.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget[index]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, target\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 20"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset class for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self,text,target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, int]:\n",
    "        \"\"\"Return item at given index.\"\"\"\n",
    "       \n",
    "        text = self.text[index]\n",
    "        target = self.target[index]\n",
    "        \n",
    "        return text, target\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "df = df[:20]\n",
    "\n",
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "  \n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "train_iter = iter(train_data)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text,_ in data_iter:\n",
    "        text = text.lower()\n",
    "        \n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b4c0680-b5b2-4419-b356-09c33fcdeae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 20 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer(text)\n\u001b[0;32m---> 21\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m     99\u001b[0m     counter\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36myield_tokens\u001b[0;34m(data_iter)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_tokens\u001b[39m(data_iter):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, _ \u001b[38;5;129;01min\u001b[39;00m data_iter:\n\u001b[1;32m     18\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer(text)\n",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"Return item at given index.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget[index]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, target\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 20"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "\n",
    "# Check the length of the DataFrame\n",
    "data_length = len(df)\n",
    "\n",
    "# Limit the DataFrame to a maximum of 20 rows\n",
    "max_rows = min(data_length, 20)  # Replace 20 with the desired length or len(df)\n",
    "df = df[:max_rows]\n",
    "\n",
    "train_data = CustomDataset(df['tweet'], df['label'])\n",
    "\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "train_iter = iter(train_data)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        text = text.lower()\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c81f9-130f-4514-b90b-05ea688db970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebaa5a-570f-407f-a06a-909ca19027f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aee33c-5be9-4bb6-856b-9679781c9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_pipeline(text):\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ba7bd-9490-44e8-b039-0945ab7b4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline(data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043acc9d-042c-4e25-b8c8-b1bda14ec688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9b093-6011-40e4-b147-96a22d77c591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bdf5a-c81c-42b3-a6e7-8cec292639a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        processed_text = text_pipeline(_text)  # Removed torch.tensor and dtype=torch.float\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(_label)\n",
    "        \n",
    "    text_list = torch.cat(text_list)  # Added torch.stack for concatenating tensors\n",
    "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "    \n",
    "    return text_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538709eb-52b2-45f9-ae4a-2e8286542eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f6e20-79f9-4a7b-b0c6-72db077071dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data[0:5]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de509e-2812-4dfc-8af9-f4748936634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text,_label = data[0]\n",
    "\n",
    "processed_text = text_pipeline(_text)  # Removed torch.tensor and dtype=torch.float\n",
    "text_list.append(processed_text)\n",
    "\n",
    "label_list.append(_label)\n",
    "\n",
    "text_list = torch.cat(text_list)  # Added torch.stack for concatenating tensors\n",
    "label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d36915-9b6c-4218-97ee-c10e515c6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_iter = iter(data)\n",
    "\n",
    "text_list, label_list = [], []\n",
    "try:\n",
    "    for _text, _label in train_iter:\n",
    "        processed_text = text_pipeline(_text)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "        label_list.append(_label)\n",
    "\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84148a7-5b87-4ebb-9c8e-0d27250aed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e74d19-98a1-4fcb-9978-00c1e2c9fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=5, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743eeec2-2b13-4b4a-ba19-a646de4054db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch,(X,y) in enumerate(dataloader):\n",
    "    print('batch:',batch,)\n",
    "    print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf871cd-ca7d-4a9f-86c7-636987c3dd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce8ab4-cb0e-4e6b-88aa-c55accb212f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bc875-e12d-47fd-b364-73fe151469fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "\n",
    "def token_gen(text):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence in a given text and yields the resulting tokens.\n",
    "\n",
    "    Args:\n",
    "        text (list[str]): A list of sentences to tokenize.\n",
    "\n",
    "    Yields:\n",
    "        list[str]: The resulting tokens from each sentence.\n",
    "    \"\"\"\n",
    "    for sent in text:\n",
    "        tokens = tokenizer(sent)\n",
    "        yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(token_gen(df['tweet']),specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])  ## to handel OOV problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89492619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "print('data type is:',type(sequence))\n",
    "\n",
    "count=0\n",
    "for ids in sequence:\n",
    "    print([num for num in ids])\n",
    "    count+=1\n",
    "    if count==10:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4f33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check how \"numericalize_tokens_from_iterator\" works\n",
    "\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "sequence = numericalize_tokens_from_iterator(vocab,[\"hi how are you\", \"what is your name?\"])\n",
    "list(next(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bd5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "# create a list to store tokenized sequences\n",
    "text = []\n",
    "for i in range(len(df)):\n",
    "    x = list(next(sequence))\n",
    "    text.append(x)\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text = pad_sequence([torch.tensor(x) for x in text], batch_first=True, padding_value=0)\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "padded_text = padded_text[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text.shape)\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2a132-5b3b-42bf-a67a-e4e684a19895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97111859-673e-4ff5-b3fb-31e368bb7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b971650-7761-4472-b165-9bbc50efd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('I will see you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab([tokens])\n",
    "\n",
    "vocab(tokenizer('I will see you!')) ## similar to the 'fit_on_texts()' in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(vocab(tokenizer('I will see you')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d703b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vocab.get_stoi())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff74e03-dfec-4371-92cc-961b71ffff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e397feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = torch.nn.Embedding(num_embeddings=len(vocab),embedding_dim=5,padding_idx=0) \n",
    "\n",
    "## if we want to add embedding for a text, we should assign the value to \"num_embeddings\" according to the \n",
    "## max 'integer_id' that is present in the tokenized text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = embedd(torch.tensor(vocab(tokenizer('I will see you nonsense!'))))\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd36066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Embedding module with the correct weight matrix size\n",
    "embedd = torch.nn.Embedding(len(vocab), 5, padding_idx=0)\n",
    "\n",
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text.shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "input_text = embedd(padded_text)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45e68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text[0].shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "embedd(padded_text[0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094939e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_text.shape)\n",
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label =df['label'].to_list()\n",
    "\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdefcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "label= torch.tensor(label)\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abadbecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlabel\u001b[49m\u001b[38;5;241m.\u001b[39munique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "len(label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(label.unique())\n",
    "\n",
    "# Define the RNNClassify module\n",
    "class RNNClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        # Define the linear layer\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize the weights of the module\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #print('rnn output shape:',output.shape)\n",
    "        #print('rnn hidden shape:',hidden.shape)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking last output of RNN\n",
    "        #print('rnn last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be011b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassify(vocab_size=VOCAB_SIZE,embed_dim=100,hidden_size=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3df556",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_text[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9326e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_text[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b953ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a5f3-0fa9-4a1f-b8c9-b754d4a0e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we restrict the length of every sequence in the padded_text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_text).shape            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b46fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bac64cb",
   "metadata": {},
   "source": [
    "### will try `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445860e4",
   "metadata": {},
   "source": [
    "#### first of all, let me fix `(X_train,y_train)` and `(X_test,y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train\n",
    "\n",
    "X_train,y_train = padded_text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42811656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744513ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e08f8-f6d7-4b6c-bf37-e4865cddd303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df_valid['tweet']))\n",
    "\n",
    "print('data type is:',type(sequence))\n",
    "\n",
    "count=0\n",
    "for ids in sequence:\n",
    "    print([num for num in ids])\n",
    "    count+=1\n",
    "    if count==10:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f33b15-72c7-4b95-a5f6-68cddbed80bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_token_ids = []\n",
    "for i in range(len(df_valid)):\n",
    "    token_id = vocab(tokenizer(df_valid['tweet'][i]))\n",
    "    valid_token_ids.append(token_id)\n",
    "    \n",
    "    \n",
    "valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_token_ids = torch.tensor(valid_token_ids) # this will throw an error, because all sequence are not of same length\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text_valid = pad_sequence([torch.tensor(x) for x in valid_token_ids], batch_first=True, padding_value=0)\n",
    "# here look, <UNK> will be assign to 0 and padding_idx will be assign also 0\n",
    "\n",
    "padded_text_valid = padded_text_valid[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text_valid.shape)\n",
    "print(padded_text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = df_valid['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = torch.tensor(label_valid)\n",
    "#label_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test,y_test\n",
    "\n",
    "X_test, y_test = padded_text_valid, label_valid\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c9b8a",
   "metadata": {},
   "source": [
    "#### Now, write the train and test loop for `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a91fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will use Batch Gradient Descent, BUT, generally we prefer Mini-Batch Gradient Descent\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc = 0,0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    X_train,y_train = X_train.to(device), y_train.to(device)\n",
    "    \n",
    "    y_logits = model(X_train)\n",
    "    #print('shape of y_logits:',y_logits.shape)\n",
    "\n",
    "    \n",
    "    # Compute loss with one-hot encoded targets\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    \n",
    "    train_loss += loss\n",
    "    train_acc += (y_logits.argmax(1) == y_train).sum().item() / len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "    \n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        \n",
    "        y_logits = model(X_test)\n",
    "\n",
    "        # Compute loss with one-hot encoded targets\n",
    "        loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "            \n",
    "        # Compute accuracy\n",
    "        test_preds = y_logits.argmax(dim=1)\n",
    "        test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "        \n",
    "        print('--'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935adac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9f2fd-1ad6-436e-b164-f151b45053f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
