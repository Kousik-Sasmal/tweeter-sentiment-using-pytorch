{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340bd39e-b706-4c48-86ee-27489d8a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Due to warning when initializing the \"spacy\" tokenizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow logging\n",
    "logging.getLogger('tensorflow').disabled = True  # disable tensorflow warning messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5e459a-e2cf-4ad8-aafe-a1f076d41349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c15953-32da-4b1f-aa45-9ae62c5185b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kousik/ABC_OTHERS/campusx/nlp_movie_data/experiment_with_PyTorch_modular/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c4cd56-f5da-4ce2-807d-b89a869abf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../artifacts/train_cleaned.csv')\n",
    "df_valid = pd.read_csv('../artifacts/valid_cleaned.csv')\n",
    "\n",
    "\n",
    "# limit df\n",
    "df = df[:10000]\n",
    "\n",
    "df_valid = df_valid[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989b6a6d-6a7b-4728-926f-d50ebb2156e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb09bcbb-be27-4fe6-9340-638052b17247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset class for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self,text,target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, int]:\n",
    "        \"\"\"Return item at given index.\"\"\"\n",
    "       \n",
    "        text = self.text[index]\n",
    "        target = self.target[index]\n",
    "        \n",
    "        return text, target\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955910c4-be17-4549-af8d-10447a2d0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = CustomDataset(df['tweet'],df['label'])\n",
    "test_data = CustomDataset(df_valid['tweet'],df_valid['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c716d85-9fcc-4686-9da2-4c4b30b7fc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('im getting on borderlands and i will murder you all ,', 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e3762f9-176c-4b24-9ad0-3317a468878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('im getting on borderlands and i will murder you all ,', 3)\n",
      "('i am coming to the borders and i will kill you all,', 3)\n",
      "('im getting on borderlands and i will kill you all,', 3)\n",
      "('im coming on borderlands and i will murder you all,', 3)\n",
      "('im getting on borderlands 2 and i will murder you me all,', 3)\n",
      "('im getting into borderlands and i can murder you all,', 3)\n",
      "(\"so i spent a few hours making something for fun. . . if you don't know i am a huge @borderlands fan and maya is one of my favorite characters. so i decided to make myself a wallpaper for my pc. . here is the original image versus the creation i made :) enjoy! pic.twitter.com/mlsi5wf9jg\", 3)\n",
      "(\"so i spent a couple of hours doing something for fun... if you don't know that i'm a huge @ borderlands fan and maya is one of my favorite characters, i decided to make a wallpaper for my pc.. here's the original picture compared to the creation i made:) have fun! pic.twitter.com / mlsi5wf9jg\", 3)\n",
      "(\"so i spent a few hours doing something for fun... if you don't know i'm a huge @ borderlands fan and maya is one of my favorite characters.\", 3)\n",
      "(\"so i spent a few hours making something for fun. . . if you don't know i am a huge rhandlerr fan and maya is one of my favorite characters. so i decided to make myself a wallpaper for my pc. . here is the original image versus the creation i made :) enjoy! pic.twitter.com/mlsi5wf9jg\", 3)\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for item in train_data:\n",
    "    count+=1\n",
    "    print(item)\n",
    "    if count==10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a6c890-9420-44e4-bf4f-41bfd3b283ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary  \n",
    "    \n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        text = text.lower()\n",
    "        \n",
    "        yield tokenizer(text)\n",
    "        \n",
    "        \n",
    "token_generator = yield_tokens(df['tweet'])\n",
    "        \n",
    "vocab = build_vocab_from_iterator(token_generator, specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4e5df-7a1f-4d4c-836c-765bb2c23a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b279ce00-7d94-426d-842f-6a2a662d4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    # Separate the texts and targets from the samples in a batch\n",
    "    texts, targets = zip(*samples)\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized_texts = [tokenizer(text.lower()) for text in texts]\n",
    "    \n",
    "    # Convert the tokenized texts to numerical values\n",
    "    text_indices = [torch.tensor(vocab(token)) for token in tokenized_texts]\n",
    "    \n",
    "    # Pad the text sequences to have the same length\n",
    "    padded_texts = torch.nn.utils.rnn.pad_sequence(text_indices, batch_first=True)\n",
    "    \n",
    "    # Convert the targets to tensors\n",
    "    target_tensor = torch.tensor(targets)\n",
    "    \n",
    "    return padded_texts, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc08bc9a-900d-4888-9c50-92d3d2ef3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_data,batch_size=3,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2beb45ad-498d-48eb-a8f4-56af6d085868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13])\n",
      "tensor([[   3,  140,  217,   15,   26,    7,    3,   88, 1141,   17,   37,    4,\n",
      "            0],\n",
      "        [   3,  128,  374,    6,    2, 2867,    7,    3,   88,  731,   17,   37,\n",
      "            4],\n",
      "        [   3,  140,  217,   15,   26,    7,    3,   88,  731,   17,   37,    4,\n",
      "            0]])\n",
      "torch.Size([3, 14])\n",
      "tensor([[   3,  140,  374,   15,   26,    7,    3,   88, 1141,   17,   37,    4,\n",
      "            0,    0],\n",
      "        [   3,  140,  217,   15,   26,   63,    7,    3,   88, 1141,   17,   38,\n",
      "           37,    4],\n",
      "        [   3,  140,  217,  221,   26,    7,    3,   85, 1141,   17,   37,    4,\n",
      "            0,    0]])\n",
      "torch.Size([3, 62])\n",
      "tensor([[  25,    3,  800,    8,  388,  292,  432,  228,   10,  101,    1,    1,\n",
      "            1,   50,   17,   39,   44,  170,    3,  128,    8,  412,  206,  541,\n",
      "            7,  968,   12,   55,   11,   21,  229,  431,    1,   25,    3,  647,\n",
      "            6,  188,  417,    8, 5416,   10,   21,  237,    1,    1,  138,   12,\n",
      "            2,  821, 1710, 5401,    2, 4766,    3,  212,  805,  485,    5, 6096,\n",
      "            0,    0],\n",
      "        [  25,    3,  800,    8,  958,   11,  292,  376,  228,   10,  101,   18,\n",
      "           50,   17,   39,   44,  170,   23,    3,   61,    8,  412,   29,   26,\n",
      "          541,    7,  968,   12,   55,   11,   21,  229,  431,    4,    3,  647,\n",
      "            6,  188,    8, 5416,   10,   21,  237,   22,  138,   32,    2,  821,\n",
      "         2388,  892,    6,    2, 4766,    3,  212,  805,   31,  101,    5,  319,\n",
      "           13, 9412],\n",
      "        [  25,    3,  800,    8,  388,  292,  376,  228,   10,  101,   18,   50,\n",
      "           17,   39,   44,  170,    3,   61,    8,  412,   29,   26,  541,    7,\n",
      "          968,   12,   55,   11,   21,  229,  431,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in data_loader:\n",
    "    count+=1\n",
    "    print(i[0].shape)\n",
    "    print(i[0])\n",
    "    if count==3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c48b3-efed-463b-ae49-148398739b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73110753-28a6-4cd4-995b-31763c7b6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=20,collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data,batch_size=20,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7b826-3097-4d28-bdcc-ab43558bac11",
   "metadata": {},
   "source": [
    "### write a class for RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "988d1684-760a-4daa-807f-798049df939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "class RNNClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size,num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize the weights of the module\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #print('rnn output shape:',output.shape)\n",
    "        #print('rnn hidden shape:',hidden.shape)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking last output of RNN\n",
    "        #print('rnn last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7a570-d6d5-4fa4-98b3-26232b87450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1df50f3-f400-485d-8ebb-1d593deac289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10690"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "274de383-a06f-4881-a228-6e2baec87700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RNNClassify(vocab_size=VOCAB_SIZE,embed_dim=64,hidden_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f5c6f0-4d77-4487-be39-2ad65be64d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "class LSTMClassify(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size,num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the LSTM layer\n",
    "        output, (hidden,cell) = self.lstm(embedded)\n",
    "        \n",
    "        \n",
    "        output = output[:, -1, :] \n",
    "        #print('LSTM last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab6836-a14c-4336-9264-ead3b10be239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcdcfdf5-806f-4d87-a8dc-2d3970e3caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes=4, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        lstm_output_size = hidden_size\n",
    "        if bidirectional:\n",
    "            lstm_output_size *= 2\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_output_size, num_classes)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        \n",
    "        # Pass the embedded input through the LSTM layer\n",
    "        output, (_, _) = self.lstm(embedded)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking the last output of the LSTM\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f51938-fc13-486a-b402-f96cd7e4454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMClassify(vocab_size=VOCAB_SIZE,embed_dim=64,hidden_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34253112-5030-416d-abfb-9df7da122945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75ab8f-3bd6-4ffb-82fc-8253b08bfc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69d432-cb4e-4263-b2f6-db58c2ff0d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a61cfb7-5e39-4602-b532-47a9ca20d620",
   "metadata": {},
   "source": [
    "### Write train-test loop for Mini-batch Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d7408-62e3-473e-813b-116f7ec02498",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_rnn.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "            \n",
    "        y_logits = model_rnn(X)\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (y_logits.argmax(1) == y).sum().item() / len(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)  # Calculate average loss per epoch\n",
    "    train_acc /= len(train_dataloader)  # Calculate average accuracy per epoch\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    \n",
    "    # evaluate model\n",
    "    with torch.inference_mode():\n",
    "        model_rnn.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "        for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
    "            \n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            \n",
    "            y_logits = model_rnn(X_test)\n",
    "\n",
    "            loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            test_preds = y_logits.argmax(dim=1)\n",
    "            test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "        \n",
    "        test_loss /= len(test_dataloader)  # Calculate average loss per epoch\n",
    "        test_acc /= len(test_dataloader)  # Calculate average accuracy per epoch\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "        \n",
    "        print('--'*25)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a45640-32ea-458b-9553-bac4bb9b85dc",
   "metadata": {},
   "source": [
    "### let's create a function for `train` and `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79dfd7d-a90a-45ef-88ec-906277bf3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               optimizer:torch.optim,\n",
    "               device=device):\n",
    "    \n",
    "    train_loss,train_acc = 0,0\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        model.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "            \n",
    "        y_logits = model(X)\n",
    "\n",
    "        loss = loss_fn(y_logits, y)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_acc += (y_logits.argmax(1) == y).sum().item() / len(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "        \n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec695b6-7cc6-4e5a-813c-da0457bc0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model,\n",
    "               dataloader,\n",
    "               loss_fn,\n",
    "               device=device):\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "        for batch, (X_test, y_test) in enumerate(dataloader):\n",
    "            \n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            \n",
    "            y_logits = model(X_test)\n",
    "\n",
    "            loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            test_preds = y_logits.argmax(dim=1)\n",
    "            test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "            \n",
    "        \n",
    "        test_loss /= len(dataloader)  \n",
    "        test_acc /= len(dataloader) \n",
    "\n",
    "\n",
    "    return test_loss,test_acc \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090bee0-128b-4468-9c78-8cfdd7ebb5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c71533-2aa1-4286-94b8-9b6a233435be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bff8817b954cfc94a2391256f45590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.3288, Train Accuracy: 0.3764\n",
      "Epoch 1/20, Test Loss: 1.4274, Test Accuracy: 0.2770\n",
      "--------------------------------------------------\n",
      "Epoch 2/20, Train Loss: 1.3375, Train Accuracy: 0.3627\n",
      "Epoch 2/20, Test Loss: 1.4525, Test Accuracy: 0.2770\n",
      "--------------------------------------------------\n",
      "Epoch 3/20, Train Loss: 1.3040, Train Accuracy: 0.3873\n",
      "Epoch 3/20, Test Loss: 1.4464, Test Accuracy: 0.2780\n",
      "--------------------------------------------------\n",
      "Epoch 4/20, Train Loss: 1.2722, Train Accuracy: 0.4011\n",
      "Epoch 4/20, Test Loss: 1.4189, Test Accuracy: 0.2790\n",
      "--------------------------------------------------\n",
      "Epoch 5/20, Train Loss: 1.2571, Train Accuracy: 0.4054\n",
      "Epoch 5/20, Test Loss: 1.4422, Test Accuracy: 0.2770\n",
      "--------------------------------------------------\n",
      "Epoch 6/20, Train Loss: 1.2235, Train Accuracy: 0.4252\n",
      "Epoch 6/20, Test Loss: 1.4311, Test Accuracy: 0.2780\n",
      "--------------------------------------------------\n",
      "Epoch 7/20, Train Loss: 1.1911, Train Accuracy: 0.4338\n",
      "Epoch 7/20, Test Loss: 1.4712, Test Accuracy: 0.2850\n",
      "--------------------------------------------------\n",
      "Epoch 8/20, Train Loss: 1.1655, Train Accuracy: 0.4449\n",
      "Epoch 8/20, Test Loss: 1.4791, Test Accuracy: 0.2920\n",
      "--------------------------------------------------\n",
      "Epoch 9/20, Train Loss: 1.1373, Train Accuracy: 0.4676\n",
      "Epoch 9/20, Test Loss: 1.4793, Test Accuracy: 0.3040\n",
      "--------------------------------------------------\n",
      "Epoch 10/20, Train Loss: 1.0862, Train Accuracy: 0.4921\n",
      "Epoch 10/20, Test Loss: 1.4953, Test Accuracy: 0.3090\n",
      "--------------------------------------------------\n",
      "Epoch 11/20, Train Loss: 1.0287, Train Accuracy: 0.5344\n",
      "Epoch 11/20, Test Loss: 1.5245, Test Accuracy: 0.3440\n",
      "--------------------------------------------------\n",
      "Epoch 12/20, Train Loss: 0.9420, Train Accuracy: 0.5840\n",
      "Epoch 12/20, Test Loss: 1.5347, Test Accuracy: 0.3380\n",
      "--------------------------------------------------\n",
      "Epoch 13/20, Train Loss: 0.8564, Train Accuracy: 0.6237\n",
      "Epoch 13/20, Test Loss: 1.5467, Test Accuracy: 0.3490\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm.parameters(),lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)\n",
    "\n",
    "epochs = 20\n",
    " \n",
    "try:\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_accuracy = train_step(model_lstm,train_dataloader,loss_fn,optimizer)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        test_loss, test_accuracy = test_step(model_lstm,test_dataloader,loss_fn)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "        print('--'*25) \n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b684d7-5cde-4f1b-b7c8-565951279cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
